# Argo-Clean ğŸŒŠğŸ“Š

A streamlined pipeline for processing Argo oceanographic data from NetCDF files to SQL databases with beautiful visualizations.

![Workflow](https://img.shields.io/badge/Workflow-Data%20Processing-blueviolet)
![Python](https://img.shields.io/badge/Python-3.8%2B-success)
![License](https://img.shields.io/badge/License-MIT-green)

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/realSUDO/argo-clean && cd argo-clean

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ“ Project Structure

```bash
argo-clean/
â”œâ”€â”€ ğŸ“‚ argo_nc_files_requests/       # Raw NetCDF4 files
â”œâ”€â”€ ğŸ“‚ argo_parquet/                 # Processed CSV & Parquet files
â”œâ”€â”€ ğŸ“‚ global_csvs/                  # Master CSV indices
â”‚   â”œâ”€â”€ ğŸ“„ argo_index_india.csv         # All-time India Argo index
â”‚   â””â”€â”€ ğŸ“„ argo_index_last3yrs.csv      # Last 3 years India Argo index
â”œâ”€â”€ ğŸ“‚ scripts/                      # Python processing scripts
â”‚   â”œâ”€â”€ ğŸ csv_generator.py             # Generate yearly CSVs
â”‚   â”œâ”€â”€ ğŸ nc_downloader_all.py         # Download all .nc files
â”‚   â”œâ”€â”€ ğŸ nc_downloader_ui.py          # Download selected date-range .nc files
â”‚   â”œâ”€â”€ ğŸ nc_to_parquet.py             # Convert .nc â†’ CSV + Parquet
â”‚   â””â”€â”€ ğŸ parquet_to_sql_db.py         # Convert Parquet â†’ SQL database
â”œâ”€â”€ ğŸ“‚ SQL-DB/                       # Generated SQL database
â””â”€â”€ ğŸ“‚ yearly_csvs/                  # Generated yearly CSVs
```

---

## âš¡ Quick Commands Cheat Sheet

| Task | Command |
|------|---------|
| Download selected NC files | `python scripts/nc_downloader_ui.py` |
| Download all NC files | `python scripts/nc_downloader_all.py` |
| Generate yearly CSVs | `python scripts/csv_generator.py` |
| Convert to Parquet | `python scripts/nc_to_parquet.py` |
| Create SQL Database | `python scripts/parquet_to_sql_db.py` |

---

## ğŸ“Š Data Processing Pipeline

### 1. ğŸ“¥ Download NetCDF Files

**Option A: Interactive Download (Recommended)**
```bash
python scripts/nc_downloader_ui.py
```
*Select specific date ranges interactively*

**Option B: Bulk Download**
```bash
python scripts/nc_downloader_all.py
```
*Downloads all files from the index*

> ğŸ’¡ **Pro Tip**: Modify `CSV_FILE` path in the scripts to use custom CSV indices

### 2. ğŸ“‹ Generate Custom CSV Files
```bash
python scripts/csv_generator.py
```
*Creates yearly aggregated CSV files for easier filtering*

### 3. ğŸ”„ Convert to Parquet Format
```bash
python scripts/nc_to_parquet.py
```
*Transforms NetCDF â†’ CSV + Parquet (optimized for analysis)*

### 4. ğŸ—ƒï¸ Create SQL Database
```bash
python scripts/parquet_to_sql_db.py
```
*Builds query-optimized SQL database from Parquet files*

---

## ğŸŒŸ Features

- **Interactive UI** for selective data download
- **Batch processing** for large datasets
- **Multiple format support** (NetCDF, CSV, Parquet, SQL)
- **Optimized storage** with Parquet compression
- **SQL-ready** for immediate analysis
- **Customizable** workflow for different regions/time periods

---

## ğŸ”§ Customization

### Using Different CSV Indices
Edit the `CSV_FILE` variable in the download scripts:
```python
# Change this path to use different CSV files
CSV_FILE = "global_csvs/your_custom_index.csv"
```

### Adding New Regions
1. Create new CSV index in `global_csvs/`
2. Update script paths to point to your new CSV
3. Run the pipeline with your custom data

---

## ğŸ“ˆ Output Structure

```
Raw Data â†’ argo_nc_files_requests/
    â†“
Processed Data â†’ argo_parquet/ (.csv + .parquet)
    â†“
Final Database â†’ SQL-DB/argo_data.db
```

---

## ğŸ¯ Use Cases

- **Oceanographic Research** - Analyze temperature, salinity patterns
- **Climate Studies** - Long-term trend analysis
- **Machine Learning** - Train models on ocean data
- **Visualization** - Create interactive maps and charts
- **Educational Purposes** - Learn about data processing pipelines

---

## ğŸ¤ Contributing

We welcome contributions! Feel free to:
- Report bugs and issues
- Suggest new features
- Submit pull requests
- Improve documentation

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸš€ Next Steps

After setting up your database:
```python
# Connect to your SQL database
import sqlite3
conn = sqlite3.connect('SQL-DB/argo_data.db')

# Start querying!
results = conn.execute("SELECT * FROM argo_data LIMIT 10")
for row in results:
    print(row)
```

---

## ğŸ’¡ Pro Tips

1. **Start small** - Use the interactive downloader for specific date ranges first
2. **Check storage** - NetCDF files can be large, ensure you have enough space
3. **Use Parquet** - For analysis, Parquet format offers better performance
4. **Backup data** - Keep copies of your raw NetCDF files
5. **Explore visually** - Use tools like Pandas, Matplotlib, and Seaborn for visualization

---

### Happy Data Processing! ğŸŒŠğŸ”

```python
# Ready to explore ocean data?
print("Dive deep into oceanographic data with Argo-Clean!")
```

*For questions and support, please open an issue on GitHub.*
